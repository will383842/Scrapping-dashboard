# üöÄ Guide de D√©ploiement Production - Scraper Pro

## Table des Mati√®res

1. [Vue d'Ensemble](#vue-densemble)
2. [Pr√©requis Syst√®me](#pr√©requis-syst√®me)
3. [Installation Rapide](#installation-rapide)
4. [Configuration Avanc√©e](#configuration-avanc√©e)
5. [D√©ploiement Production](#d√©ploiement-production)
6. [Monitoring et Maintenance](#monitoring-et-maintenance)
7. [S√©curit√©](#s√©curit√©)
8. [D√©pannage](#d√©pannage)
9. [FAQ](#faq)

---

## Vue d'Ensemble

Scraper Pro est une plateforme de scraping web professionnelle avec dashboard Streamlit, worker Scrapy/Playwright, et base de donn√©es PostgreSQL. Cette version 2.0 est enti√®rement production-ready avec monitoring, backup automatique, et scripts d'automatisation.

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Dashboard     ‚îÇ    ‚îÇ     Worker       ‚îÇ    ‚îÇ   PostgreSQL    ‚îÇ
‚îÇ   (Streamlit)   ‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ (Scrapy+Playwright)‚îÇ‚óÑ‚îÄ‚îÄ‚ñ∫‚îÇ   Database      ‚îÇ
‚îÇ   Port: 8501    ‚îÇ    ‚îÇ   Background     ‚îÇ    ‚îÇ   Port: 5432    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñ≤                        ‚ñ≤                        ‚ñ≤
         ‚îÇ                        ‚îÇ                        ‚îÇ
         ‚ñº                        ‚ñº                        ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Docker Network                               ‚îÇ
‚îÇ                   scraper-pro-internal                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Fonctionnalit√©s Cl√©s

- ‚úÖ **Interface Web Moderne** - Dashboard Streamlit intuitif
- ‚úÖ **Scraping Avanc√©** - Scrapy + Playwright avec JavaScript
- ‚úÖ **Gestion de Proxies** - Import/export, test automatique, rotation
- ‚úÖ **Sessions Authentifi√©es** - Upload storage states, validation
- ‚úÖ **Export Donn√©es** - CSV, Excel, PDF avec formatage
- ‚úÖ **Monitoring Temps R√©el** - M√©triques, alertes, health checks
- ‚úÖ **Backup Automatique** - Sauvegarde programm√©e avec r√©tention
- ‚úÖ **Scripts PowerShell** - Automatisation compl√®te Windows
- ‚úÖ **Production Ready** - SSL, reverse proxy, logging, s√©curit√©

---

## Pr√©requis Syst√®me

### Configuration Minimale

| Composant    | Minimum    | Recommand√©  | Production           |
| ------------ | ---------- | ----------- | -------------------- |
| **OS**       | Windows 10 | Windows 11  | Windows Server 2019+ |
| **RAM**      | 8 GB       | 16 GB       | 32 GB+               |
| **CPU**      | 4 cores    | 8 cores     | 16+ cores            |
| **Stockage** | 50 GB SSD  | 100 GB NVMe | 500 GB+ NVMe         |
| **R√©seau**   | 100 Mbps   | 1 Gbps      | 10 Gbps              |

### Logiciels Requis

- **Docker Desktop** 4.20+ avec WSL2
- **PowerShell** 5.1+ (Windows PowerShell ou PowerShell 7+)
- **Git** (optionnel, pour le d√©veloppement)

### Ports Utilis√©s

| Service    | Port   | Description                |
| ---------- | ------ | -------------------------- |
| Dashboard  | 8501   | Interface web principale   |
| PostgreSQL | 5432   | Base de donn√©es            |
| Nginx      | 80/443 | Reverse proxy (production) |
| Prometheus | 9090   | Monitoring (optionnel)     |
| Grafana    | 3000   | Dashboards (optionnel)     |
| Redis      | 6379   | Cache (optionnel)          |

---

## Installation Rapide

### Option 1: Installation Automatis√©e (Recommand√©e)

```powershell
# 1. T√©l√©charger ou cloner le projet
git clone https://github.com/your-repo/scraper-pro.git
cd scraper-pro

# 2. Ex√©cuter l'installation automatique
.\scripts\setup.ps1

# 3. Attendre la fin de l'installation (5-10 minutes)
# 4. Ouvrir http://localhost:8501
```

### Option 2: Installation Manuelle

```powershell
# 1. Cr√©er le dossier projet
mkdir C:\scraper-pro
cd C:\scraper-pro

# 2. Copier tous les fichiers du projet

# 3. Configurer l'environnement
cp .env.example .env
# √âditer .env avec vos param√®tres

# 4. Construire et d√©marrer
docker compose up -d

# 5. V√©rifier le statut
.\scripts\manage.ps1 status
```

### V√©rification Installation

```powershell
# Test de sant√© complet
.\scripts\manage.ps1 health

# V√©rifier les services
docker compose ps

# Acc√©der au dashboard
start http://localhost:8501
```

---

## Configuration Avanc√©e

### Fichier .env Principal

```bash
# ============================================================================
# CONFIGURATION PRODUCTION SCRAPER PRO
# ============================================================================

# Base de donn√©es
POSTGRES_DB=scraper_pro
POSTGRES_USER=scraper_admin
POSTGRES_PASSWORD=your_secure_password_here
POSTGRES_HOST=db
POSTGRES_PORT=5432

# Dashboard
DASHBOARD_USERNAME=admin
DASHBOARD_PASSWORD=your_dashboard_password
DASHBOARD_PORT=8501

# S√©curit√©
SECRET_KEY=your_secret_key_32_chars_minimum
JWT_SECRET=your_jwt_secret_key_here
FORCE_HTTPS=true

# Performance
SCRAPY_CONCURRENT_REQUESTS=16
SCRAPY_DOWNLOAD_DELAY=0.5
JS_PAGES_LIMIT=5000
MAX_PAGES_PER_DOMAIN=100
WORKER_TIMEOUT=3600

# Monitoring
ENABLE_MONITORING=true
PROMETHEUS_PORT=9090
GRAFANA_PORT=3000
GRAFANA_PASSWORD=admin123

# SSL/TLS
SSL_CERT_PATH=./ssl/fullchain.pem
SSL_KEY_PATH=./ssl/privkey.pem

# Backup
BACKUP_SCHEDULE=daily
BACKUP_RETENTION_DAYS=30
AUTO_BACKUP_ENABLED=true

# Email Alerts
ENABLE_EMAIL_ALERTS=true
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USER=alerts@company.com
SMTP_PASSWORD=app_password
ALERT_RECIPIENTS=admin@company.com,devops@company.com
```

### Configuration PostgreSQL Optimis√©e

Cr√©er `config/postgresql.conf`:

```bash
# Performance
shared_buffers = 1GB                    # 25% de la RAM
effective_cache_size = 3GB              # 75% de la RAM
work_mem = 32MB                         # RAM / max_connections
maintenance_work_mem = 256MB
max_connections = 100

# WAL
wal_buffers = 32MB
checkpoint_completion_target = 0.9
max_wal_size = 2GB
min_wal_size = 512MB

# Query planner
random_page_cost = 1.1
effective_io_concurrency = 200

# Logging
log_statement = 'mod'
log_min_duration_statement = 1000
log_checkpoints = on
log_connections = on
log_disconnections = on
```

### Configuration Nginx Production

Cr√©er `nginx/conf.d/scraper-pro.conf`:

```nginx
# Upstream pour load balancing
upstream dashboard {
    server dashboard:8501 weight=1 max_fails=3 fail_timeout=30s;
    keepalive 32;
}

# HTTP redirect to HTTPS
server {
    listen 80;
    server_name your-domain.com;
    return 301 https://$server_name$request_uri;
}

# HTTPS main server
server {
    listen 443 ssl http2;
    server_name your-domain.com;

    # SSL Configuration
    ssl_certificate /etc/nginx/ssl/fullchain.pem;
    ssl_certificate_key /etc/nginx/ssl/privkey.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256;
    ssl_prefer_server_ciphers off;

    # Security headers
    add_header X-Frame-Options DENY;
    add_header X-Content-Type-Options nosniff;
    add_header X-XSS-Protection "1; mode=block";
    add_header Strict-Transport-Security "max-age=31536000" always;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_types text/css application/javascript application/json;

    # Rate limiting
    limit_req_zone $binary_remote_addr zone=dashboard:10m rate=10r/s;

    location / {
        limit_req zone=dashboard burst=20 nodelay;

        proxy_pass http://dashboard;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # WebSocket support
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";

        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;
    }

    # Health check endpoint
    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

---

## D√©ploiement Production

### 1. Pr√©paration Serveur

```powershell
# Cr√©er utilisateur d√©di√© (optionnel)
net user scraperuser "SecurePassword123!" /add
net localgroup "Users" scraperuser /add

# Cr√©er structure de dossiers
mkdir C:\scraper-pro\{data,logs,backups,ssl,monitoring}
```

### 2. Configuration SSL

```powershell
# G√©n√©rer certificats auto-sign√©s (d√©veloppement)
.\scripts\generate-ssl.ps1

# Ou copier certificats Let's Encrypt (production)
copy "C:\Certbot\live\domain.com\fullchain.pem" "ssl\"
copy "C:\Certbot\live\domain.com\privkey.pem" "ssl\"
```

### 3. D√©ploiement avec Profiles

```powershell
# Production compl√®te avec monitoring
docker compose --profile production --profile monitoring up -d

# Production avec backup automatique
docker compose --profile production --profile backup up -d

# Production + Cache Redis
docker compose --profile production --profile cache up -d
```

### 4. Configuration Firewall Windows

```powershell
# Ouvrir les ports n√©cessaires
netsh advfirewall firewall add rule name="Scraper Pro HTTP" dir=in action=allow protocol=TCP localport=80
netsh advfirewall firewall add rule name="Scraper Pro HTTPS" dir=in action=allow protocol=TCP localport=443
netsh advfirewall firewall add rule name="Scraper Pro Dashboard" dir=in action=allow protocol=TCP localport=8501
```

### 5. Configuration Windows Service (Optionnel)

Cr√©er `scripts/install-service.ps1`:

```powershell
# Installation comme service Windows avec NSSM
$serviceName = "ScraperPro"
$serviceDescription = "Scraper Pro - Web Scraping Platform"
$workingDir = "C:\scraper-pro"
$executable = "docker"
$arguments = "compose up"

# T√©l√©charger et installer NSSM
$nssmUrl = "https://nssm.cc/release/nssm-2.24.zip"
# ... installation NSSM ...

# Cr√©er le service
nssm install $serviceName $executable
nssm set $serviceName Arguments $arguments
nssm set $serviceName AppDirectory $workingDir
nssm set $serviceName Description "$serviceDescription"
nssm set $serviceName Start SERVICE_AUTO_START

# D√©marrer le service
net start $serviceName
```

---

## Monitoring et Maintenance

### Dashboard Monitoring

Une fois Prometheus et Grafana activ√©s:

1. **Prometheus**: http://localhost:9090
2. **Grafana**: http://localhost:3000 (admin/admin123)

### M√©triques Surveill√©es

- **Jobs**: Taux de succ√®s, dur√©e moyenne, queue size
- **Contacts**: Extraction rate, d√©duplication, validation
- **Proxies**: Sant√©, temps de r√©ponse, rotation
- **Syst√®me**: CPU, RAM, disque, r√©seau
- **Base de donn√©es**: Connexions, requ√™tes lentes, taille

### Alertes Configur√©es

```yaml
# monitoring/rules/scraper.yml
groups:
  - name: scraper-pro
    rules:
      - alert: JobFailureRate
        expr: rate(scraper_jobs_failed[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Taux d'√©chec jobs √©lev√©"

      - alert: DatabaseConnections
        expr: postgresql_connections > 80
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Trop de connexions DB"

      - alert: ProxyHealth
        expr: scraper_proxies_healthy / scraper_proxies_total < 0.7
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Moins de 70% des proxies fonctionnels"
```

### Scripts de Maintenance Quotidienne

```powershell
# T√¢che programm√©e Windows
schtasks /create /tn "Scraper Pro Daily Maintenance" /tr "C:\scraper-pro\scripts\daily-maintenance.ps1" /sc daily /st 03:00

# Script daily-maintenance.ps1
.\scripts\manage.ps1 backup
.\scripts\manage.ps1 clean
.\scripts\manage.ps1 health
```

---

## S√©curit√©

### Checklist S√©curit√© Production

- [ ] **Mots de passe forts** dans .env (20+ caract√®res)
- [ ] **SSL/TLS activ√©** avec certificats valides
- [ ] **Firewall configur√©** - ports minimum ouverts
- [ ] **Docker rootless** (optionnel mais recommand√©)
- [ ] **Network segmentation** - r√©seaux Docker s√©par√©s
- [ ] **Logs s√©curis√©s** - pas de credentials dans les logs
- [ ] **Backup chiffr√©** - sauvegarde avec chiffrement
- [ ] **Updates r√©guli√®res** - images Docker et OS
- [ ] **Monitoring s√©curit√©** - d√©tection d'intrusion
- [ ] **Access control** - authentification renforc√©e

### Configuration S√©curit√© Avanc√©e

```bash
# .env - Variables s√©curit√©
SECURITY_LEVEL=high
ENABLE_2FA=true
SESSION_TIMEOUT=3600
MAX_LOGIN_ATTEMPTS=5
BLOCK_DURATION=1800

# Rate limiting
RATE_LIMIT_ENABLED=true
RATE_LIMIT_REQUESTS=100
RATE_LIMIT_WINDOW=3600

# Audit logs
AUDIT_LOGS_ENABLED=true
AUDIT_LOGS_RETENTION=90
```

### Durcissement Docker

```yaml
# docker-compose.override.yml pour production
services:
  dashboard:
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
    user: "1001:1001"

  worker:
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
```

---

## D√©pannage

### Probl√®mes Courants

#### 1. Services ne d√©marrent pas

```powershell
# V√©rifier les logs
.\scripts\manage.ps1 logs

# V√©rifier l'espace disque
Get-WmiObject -Class Win32_LogicalDisk | Select-Object DeviceID, @{Name="FreeSpace(GB)";Expression={[math]::Round($_.FreeSpace/1GB,2)}}

# Red√©marrer Docker Desktop
Restart-Service docker
```

#### 2. Base de donn√©es inaccessible

```powershell
# Tester la connexion
docker compose exec db pg_isready -U scraper_admin -d scraper_pro

# V√©rifier les logs DB
docker compose logs db

# Reset base de donn√©es (ATTENTION: perte de donn√©es)
docker compose down -v
docker compose up -d db
```

#### 3. Dashboard lent ou inaccessible

```powershell
# V√©rifier les ressources
docker stats scraper-pro-dashboard

# Red√©marrer uniquement le dashboard
docker compose restart dashboard

# V√©rifier la configuration Streamlit
docker compose exec dashboard streamlit --version
```

#### 4. Worker n'ex√©cute pas les jobs

```powershell
# V√©rifier le scheduler
.\scripts\manage.ps1 stats

# Voir les logs worker en temps r√©el
docker compose logs -f worker

# Red√©marrer le worker
docker compose restart worker
```

### Logs et Debugging

```powershell
# Logs d√©taill√©s avec timestamps
docker compose logs --timestamps --follow

# Logs d'un service specifique
docker compose logs worker --tail=100

# Exporter les logs
docker compose logs --no-color > scraper-logs-$(Get-Date -Format 'yyyyMMdd-HHmmss').txt
```

### Recovery Procedures

#### Recovery Base de Donn√©es

```powershell
# Restaurer depuis backup automatique
.\scripts\manage.ps1 restore

# Recovery manuel
docker compose down
docker volume rm scraper-pro_pgdata
docker compose up -d db
# Attendre initialisation
docker compose exec -T db psql -U scraper_admin -d scraper_pro < backup.sql
```

#### Recovery Complet Syst√®me

```powershell
# Sauvegarde pr√©ventive
.\scripts\manage.ps1 backup

# Reset complet avec restauration
docker compose down -v
docker system prune -a -f
.\scripts\setup.ps1 -Force
.\scripts\manage.ps1 restore "backup_file.zip"
```

---

## FAQ

### Q: Puis-je utiliser Scraper Pro sans Docker?

**R**: Non recommand√©. Docker garantit la coh√©rence de l'environnement et simplifie grandement le d√©ploiement. Pour un environnement sans Docker, il faudrait installer manuellement Python, PostgreSQL, et toutes les d√©pendances.

### Q: Comment augmenter les performances?

**R**:

1. Augmenter `SCRAPY_CONCURRENT_REQUESTS` dans .env
2. Optimiser la configuration PostgreSQL
3. Ajouter plus de RAM aux containers Docker
4. Utiliser des SSD NVMe pour le stockage
5. Activer le cache Redis

### Q: Puis-je d√©ployer sur Linux?

**R**: Les scripts PowerShell sont sp√©cifiques Windows, mais Docker Compose fonctionne sur Linux. Il faudrait adapter les scripts en bash et ajuster quelques configurations.

### Q: Comment g√©rer plusieurs environnements (dev/staging/prod)?

**R**: Cr√©er plusieurs fichiers .env:

```powershell
# D√©veloppement
docker compose --env-file .env.dev up -d

# Production
docker compose --env-file .env.prod --profile production up -d
```

### Q: Comment int√©grer avec mon CI/CD?

**R**: Exemple GitLab CI:

```yaml
deploy:
  script:
    - docker compose pull
    - docker compose up -d --force-recreate
    - .\scripts\manage.ps1 health
```

### Q: Combien de proxies puis-je g√©rer?

**R**: Test√© jusqu'√† 10,000 proxies. Au-del√†, consid√©rer un sharding de la base de donn√©es ou un syst√®me distribu√©.

### Q: Les donn√©es sont-elles sauvegard√©es automatiquement?

**R**: Oui, si `AUTO_BACKUP_ENABLED=true`. Sauvegardes quotidiennes par d√©faut avec r√©tention de 30 jours.

---

## Support et Ressources

- **Documentation**: [docs/](./docs/)
- **Issues**: [GitHub Issues](https://github.com/your-repo/scraper-pro/issues)
- **Discussions**: [GitHub Discussions](https://github.com/your-repo/scraper-pro/discussions)
- **Wiki**: [GitHub Wiki](https://github.com/your-repo/scraper-pro/wiki)

---

**Scraper Pro v2.0** - Production Ready Web Scraping Platform  
¬© 2025 - Tous droits r√©serv√©s
