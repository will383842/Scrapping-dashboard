# ============================================================================
# SCRAPER PRO - DOCKER COMPOSE PRODUCTION (CORRIGÉ)
# Version: 2.0 Production-Ready - Fix connexions DB
# ============================================================================

version: '3.8'

services:
  # ==========================================================================
  # DATABASE - PostgreSQL avec configuration stabilisée
  # ==========================================================================
  db:
    image: postgres:15-alpine
    container_name: scraper-pro-db
    hostname: scraper-db
    env_file: .env
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-scraper_pro}
      POSTGRES_USER: ${POSTGRES_USER:-scraper_admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_INITDB_ARGS: "--encoding=UTF8 --locale=C"
      # Variables pour éviter les erreurs de connexion
      PGUSER: ${POSTGRES_USER:-scraper_admin}
      PGDATABASE: ${POSTGRES_DB:-scraper_pro}
    volumes:
      # Données persistantes
      - pgdata:/var/lib/postgresql/data
      # Scripts d'initialisation
      - ./db/init.sql:/docker-entrypoint-initdb.d/01-init.sql:ro
      # Configuration personnalisée
      - ./config/postgresql.conf:/etc/postgresql/postgresql.conf:ro
      # Backups
      - ./backups:/backups
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    # Health check robuste
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -p 5432 -U ${POSTGRES_USER:-scraper_admin} -d ${POSTGRES_DB:-scraper_pro}"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 30s
    restart: unless-stopped
    networks:
      - scraper-network
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    # Commande avec configuration personnalisée
    command: >
      postgres 
      -c config_file=/etc/postgresql/postgresql.conf
      -c logging_collector=off
      -c log_destination=stderr

  # ==========================================================================
  # WORKER - Scraping Engine avec gestion d'erreur améliorée
  # ==========================================================================
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
      args:
        PYTHON_VERSION: 3.11
        PLAYWRIGHT_VERSION: 1.47.0
    image: scraper-pro-worker:latest
    container_name: scraper-pro-worker
    hostname: scraper-worker
    env_file: .env
    environment:
      SCRAPY_SETTINGS_MODULE: scraper.settings
      PYTHONUNBUFFERED: 1
      PYTHONPATH: /app
      # Configuration DB avec timeouts généreux
      POSTGRES_HOST: db
      POSTGRES_CONNECT_TIMEOUT: 30
      POSTGRES_STATEMENT_TIMEOUT: 300000
    depends_on:
      db:
        condition: service_healthy
    volumes:
      - ./sessions:/app/sessions:rw
      - ./logs:/app/logs:rw
      - ./config:/app/config:ro
      - playwright-cache:/home/scraper/.cache
    restart: unless-stopped
    networks:
      - scraper-network
    # Health check moins agressif
    healthcheck:
      test: |
        python -c "
        import psycopg2
        import os
        import sys
        try:
          conn = psycopg2.connect(
            host='db', 
            port=5432,
            database=os.environ.get('POSTGRES_DB', 'scraper_pro'),
            user=os.environ.get('POSTGRES_USER', 'scraper_admin'), 
            password=os.environ.get('POSTGRES_PASSWORD'),
            connect_timeout=10
          )
          conn.close()
          sys.exit(0)
        except Exception as e:
          print(f'Health check failed: {e}')
          sys.exit(1)
        "
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "3"

  # ==========================================================================
  # DASHBOARD - Interface Web Streamlit
  # ==========================================================================
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
      args:
        PYTHON_VERSION: 3.11
    image: scraper-pro-dashboard:latest
    container_name: scraper-pro-dashboard
    hostname: scraper-dashboard
    env_file: .env
    environment:
      # Configuration Streamlit
      STREAMLIT_SERVER_PORT: 8501
      STREAMLIT_SERVER_ADDRESS: 0.0.0.0
      STREAMLIT_SERVER_HEADLESS: true
      STREAMLIT_BROWSER_GATHER_USAGE_STATS: false
      # Base de données avec timeouts généreux
      POSTGRES_HOST: db
      POSTGRES_CONNECT_TIMEOUT: 30
      POSTGRES_STATEMENT_TIMEOUT: 300000
    depends_on:
      db:
        condition: service_healthy
    ports:
      - "${DASHBOARD_PORT:-8501}:8501"
    volumes:
      - ./sessions:/app/sessions:rw
      - ./logs:/app/logs:rw  
      - ./backups:/app/backups:rw
      - ./config:/app/config:ro
    restart: unless-stopped
    networks:
      - scraper-network
    # Health check Streamlit plus tolérant
    healthcheck:
      test: |
        curl -f --max-time 10 http://localhost:8501/_stcore/health || 
        python -c "
        import requests
        import sys
        try:
          r = requests.get('http://localhost:8501/_stcore/health', timeout=10)
          sys.exit(0 if r.status_code == 200 else 1)
        except Exception as e:
          print(f'Dashboard health check failed: {e}')
          sys.exit(1)
        "
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    logging:
      driver: json-file
      options:
        max-size: "20m"
        max-file: "3"

# ==============================================================================
# VOLUMES - Persistance des données
# ==============================================================================
volumes:
  # Base de données PostgreSQL
  pgdata:
    driver: local

  # Cache Playwright
  playwright-cache:
    driver: local

# ==============================================================================
# NETWORKS - Segmentation réseau
# ==============================================================================
networks:
  # Réseau interne pour communication entre services
  scraper-network:
    driver: bridge
    name: scraper-pro-internal